{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project 1**\n",
    "\n",
    "Work in groups of 2-5. Prepare ~10 slides where one slide describes your setup and analysis, main results, challenges/suprises and take-home message.\n",
    "\n",
    "Everyone should do the first part of the project. You can then choose from one of the three themes for part 2.\n",
    "\n",
    "For the first part of the project, you will be working on a high-dimensional data from the cancer genom atlas (TCGA). The data matrices contain gene expression (relates to how much of a gene product (protein) is being produced) for 2000 randomly selected genes over 2887 cancer samples. There are 6 types of cancers in this data set. You don't need to know any bio for the project - just think of this as a 6 class data set with 2000 features.\n",
    "\n",
    "You will notice that the classes are quite imbalanced. There are a lot of breast cancer samples and very few uterine samples, etc.\n",
    "\n",
    "All of the project themes aim at you exploring how to make a method \"crash\" in some way - meaning that you illustrate when a particular method or approach would be a bad choice, or simple a situation when things might not work out as hoped for.\n",
    "\n",
    "For all tasks, you have to repeat the exercise in order to be able to draw conclusions. That is, one single run of a data analysis task or simulation has very limited information so repeat a few times to ensure you are not drawing conclusions based on a random \"fluke\".\n",
    "\n",
    "**Part 1 - Dimension reduction and predictive modeling**\n",
    "\n",
    "Everyone should do this task.\n",
    "\n",
    "1. High-dimensional data tend to be \"data hungry\". For some methods, high-dimensionality can results in a large number of parameters having to be estimated and as a consequence you might high estimation variance. It is therefore quite common to try to reduce the dimensionality of the problem prior to modeling. (An alternative to this is filtering where you remove a (possibly large) subset of features before modeling.\n",
    "\n",
    "2. In class we have so-far discussed a linear dimension reduction technique, PCA and some basic filtering based on e.g. variance, t or F-tests.\n",
    "\n",
    "3. You should explore at least 3 different classifiers of different character, from flexible to rigid (e.g. small k to large k in k-nearest neighbors). Feel free to try any other classifier you like as long as you explain if they are flexible or rigid.\n",
    "\n",
    "4. Perform cross-validation to select the number of principal components that optimizing predictive performance.\n",
    "Perform cross-validation to select the number features (genes) that optimizing predictive performance.\n",
    "Demonstrate the optimism of training by comparing the difference between training error, cross-validation error after selection of optimal PCs or features, and test error performance. Discuss the difference for the fleixble and rigid classifiers in terms of optimism.\n",
    "Repeat the above for 3 different size training sets and discuss the results.\n",
    "\n",
    "Voluntary - something to think about\n",
    "\n",
    "1. Can you construct a data set where PCA dimension reduction should improve classification performance?\n",
    "2. Can you construct a data set where PCA dimension reduction should reduce classification performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[0;32m     14\u001b[0m     file_path_labels, \n\u001b[0;32m     15\u001b[0m     delim_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Use whitespace as the delimiter\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,               \u001b[38;5;66;03m# Use the first line as the header\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m           \u001b[38;5;66;03m# Specify the quote character used\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Merge the original DataFrame with the new DataFrame\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, \u001b[43mnew_df\u001b[49m, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path_data = \"TCGAdata.txt\"\n",
    "file_path_labels = \"TCGAlabels.txt\"\n",
    "# Read the data into a DataFrame\n",
    "df_data = pd.read_csv(\n",
    "    file_path_data, \n",
    "    delim_whitespace=True,  # Use whitespace as the delimiter\n",
    "    header=0,               # Use the first line as the header\n",
    "    quotechar='\"'           # Specify the quote character used\n",
    ")\n",
    "\n",
    "# Read the labels into a DataFrame\n",
    "df_labels = pd.read_csv(\n",
    "    file_path_labels, \n",
    "    delim_whitespace=True,  # Use whitespace as the delimiter\n",
    "    header=0,               # Use the first line as the header\n",
    "    quotechar='\"'           # Specify the quote character used\n",
    ")\n",
    "\n",
    "# Merge the original DataFrame with the new DataFrame\n",
    "merged_df = pd.merge(df_data, df_labels, left_index=True, right_index=True, how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
