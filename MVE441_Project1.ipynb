{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project 1**\n",
    "\n",
    "Work in groups of 2-5. Prepare ~10 slides where one slide describes your setup and analysis, main results, challenges/suprises and take-home message.\n",
    "\n",
    "Everyone should do the first part of the project. You can then choose from one of the three themes for part 2.\n",
    "\n",
    "For the first part of the project, you will be working on a high-dimensional data from the cancer genom atlas (TCGA). The data matrices contain gene expression (relates to how much of a gene product (protein) is being produced) for 2000 randomly selected genes over 2887 cancer samples. There are 6 types of cancers in this data set. You don't need to know any bio for the project - just think of this as a 6 class data set with 2000 features.\n",
    "\n",
    "You will notice that the classes are quite imbalanced. There are a lot of breast cancer samples and very few uterine samples, etc.\n",
    "\n",
    "All of the project themes aim at you exploring how to make a method \"crash\" in some way - meaning that you illustrate when a particular method or approach would be a bad choice, or simple a situation when things might not work out as hoped for.\n",
    "\n",
    "For all tasks, you have to repeat the exercise in order to be able to draw conclusions. That is, one single run of a data analysis task or simulation has very limited information so repeat a few times to ensure you are not drawing conclusions based on a random \"fluke\".\n",
    "\n",
    "**Part 1 - Dimension reduction and predictive modeling**\n",
    "\n",
    "*Everyone should do this task.*\n",
    "\n",
    "1. High-dimensional data tend to be \"data hungry\". For some methods, high-dimensionality can results in a large number of parameters having to be estimated and as a consequence you might high estimation variance. It is therefore quite common to try to reduce the dimensionality of the problem prior to modeling. (An alternative to this is filtering where you remove a (possibly large) subset of features before modeling.\n",
    "\n",
    "2. In class we have so-far discussed a linear dimension reduction technique, PCA and some basic filtering based on e.g. variance, t or F-tests.\n",
    "\n",
    "3. You should explore at least 3 different classifiers of different character, from flexible to rigid (e.g. small k to large k in k-nearest neighbors). Feel free to try any other classifier you like as long as you explain if they are flexible or rigid.\n",
    "\n",
    "4. Perform cross-validation to select the number of principal components that optimizing predictive performance.\n",
    "Perform cross-validation to select the number features (genes) that optimizing predictive performance.\n",
    "Demonstrate the optimism of training by comparing the difference between training error, cross-validation error after selection of optimal PCs or features, and test error performance. Discuss the difference for the fleixble and rigid classifiers in terms of optimism.\n",
    "Repeat the above for 3 different size training sets and discuss the results.\n",
    "\n",
    "*Voluntary - something to think about*\n",
    "\n",
    "1. Can you construct a data set where PCA dimension reduction should improve classification performance?\n",
    "2. Can you construct a data set where PCA dimension reduction should reduce classification performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2887, 2001)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_data = \"TCGAdata.txt\"\n",
    "file_path_labels = \"TCGAlabels.txt\"\n",
    "\n",
    "df_data = pd.read_csv(\n",
    "    file_path_data, \n",
    "    delim_whitespace=True,  # Use whitespace as the delimiter\n",
    "    header=0,               # Use the first line as the header\n",
    "    quotechar='\"'           # Specify the quote character used\n",
    ")\n",
    "\n",
    "df_labels = pd.read_csv(\n",
    "    file_path_labels, \n",
    "    delim_whitespace=True,  \n",
    "    header=0,               \n",
    "    quotechar='\"'           \n",
    ")\n",
    "\n",
    "df = pd.merge(df_data, df_labels, left_index=True, right_index=True, how='left')\n",
    "\n",
    "df.head(1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2887, 2001)\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.715940  1.217289 -0.778058 -0.449348 -0.643175  1.626182  1.326347   \n",
      "1 -4.888000 -0.103819 -0.778058 -1.891912 -0.949149 -0.077379 -0.108216   \n",
      "2 -1.253872 -0.035045 -0.778058  0.010001 -0.108216  0.463993  0.623136   \n",
      "3 -0.853313 -0.372010 -0.778058 -0.316451 -0.365051 -0.107704 -1.329117   \n",
      "4  2.059144  0.439639 -0.778058 -0.288603  0.668848 -1.003692  0.080642   \n",
      "\n",
      "         V8         V9       V10  ...     V1992     V1993     V1994     V1995  \\\n",
      "0  0.689276  14.728014 -0.169078  ... -0.569072  0.197018 -1.202524 -2.034733   \n",
      "1 -0.360422  -0.123994 -0.169078  ... -0.445065  1.678946 -1.202524 -3.909225   \n",
      "2 -1.125541   9.050189 -0.169078  ... -0.567281  0.036671 -1.202524 -0.363149   \n",
      "3  0.273341  -0.123994 -0.169078  ... -0.935919 -0.663160 -1.202524  0.720660   \n",
      "4 -0.796653  -0.123994 -0.169078  ... -0.048958  0.438989 -0.364075 -0.696148   \n",
      "\n",
      "      V1996     V1997     V1998     V1999     V2000    x  \n",
      "0 -1.245852 -0.317109 -0.712213 -0.706517  0.862098  GBM  \n",
      "1  1.220923 -0.884353 -0.712213 -0.706517  1.682080  GBM  \n",
      "2  0.053091 -0.508003 -0.712213 -0.706517 -0.735057  GBM  \n",
      "3  0.958971 -0.158030 -0.712213  1.283158  0.896130  GBM  \n",
      "4 -0.438439  0.488074 -0.712213 -0.092466 -1.507395  GBM  \n",
      "\n",
      "[5 rows x 2001 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "# Assuming your dataset is in a CSV file named 'data.csv'\n",
    "# Replace 'data.csv' with the actual path to your dataset file\n",
    "#df = pd.read_csv('data.csv', skiprows=1)\n",
    "\n",
    "# Assuming the last column is the label\n",
    "features = df.iloc[:, :-1]  # Select only feature columns\n",
    "labels = df.iloc[:, -1]  # Select the label column\n",
    "print(df.shape)\n",
    "\n",
    "# Centering and normalization\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the normalized features back to a DataFrame\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "# Reattach the labels to the features DataFrame\n",
    "final_df = pd.concat([features_scaled_df, labels.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Show the first few rows of the final, normalized dataset\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2309, 2000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = final_df.iloc[:, :-1] #features\n",
    "y = final_df.iloc[:, -1] #Labels\n",
    "\n",
    "#Split data into 80-20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42 )\n",
    "\n",
    "print(X_train.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
